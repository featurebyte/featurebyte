"""
Module for managing physical feature table cache as well as metadata storage.
"""
from typing import Dict, List, Optional, Tuple, cast

from bson import ObjectId
from sqlglot import expressions

from featurebyte.enum import InternalName, MaterializedTableNamePrefix, SourceType
from featurebyte.exception import DocumentNotFoundError
from featurebyte.models.base import PydanticObjectId
from featurebyte.models.feature_store import FeatureStoreModel
from featurebyte.models.feature_table_cache_metadata import (
    CachedFeatureDefinition,
    FeatureTableCacheMetadataModel,
)
from featurebyte.models.observation_table import ObservationTableModel
from featurebyte.query_graph.graph import QueryGraph
from featurebyte.query_graph.node import Node
from featurebyte.query_graph.node.schema import TableDetails
from featurebyte.query_graph.sql.adapter import get_sql_adapter
from featurebyte.query_graph.sql.common import (
    get_dialect_from_source_type,
    get_qualified_column_identifier,
    quoted_identifier,
    sql_to_string,
)
from featurebyte.query_graph.transform.definition import DefinitionHashExtractor
from featurebyte.query_graph.transform.offline_store_ingest import extract_dtype_from_graph
from featurebyte.service.entity_validation import EntityValidationService
from featurebyte.service.feature_list import FeatureListService
from featurebyte.service.feature_table_cache_metadata import FeatureTableCacheMetadataService
from featurebyte.service.historical_features import get_historical_features
from featurebyte.service.namespace_handler import NamespaceHandler
from featurebyte.service.session_manager import SessionManagerService
from featurebyte.service.tile_cache import TileCacheService
from featurebyte.session.base import BaseSession


def alter_table_add_columns_sql(
    table: expressions.Table,
    columns: List[expressions.ColumnDef],
    source_type: SourceType,
) -> str:
    """Generate ALTER TABLE ADD COLUMN sql.
    sqlglot doesn't support it well at the moment of writing this code
    - for snowflake only 1 column alter commands work, multiple columns result into invalid sql
    - for spark any sql generated by sqlglot is invalid

    Parameters
    ----------
    table: expressions.Table
        Table to alter
    columns: List[expressions.ColumnDef]
        List of columns to add
    source_type: SourceType
        DWH Source type

    Returns
    -------
    str
        ALTER sql statement
    """
    alter_table_sql = f"ALTER TABLE {sql_to_string(table, source_type=source_type)}"

    if get_dialect_from_source_type(source_type) == "snowflake":
        first = sql_to_string(columns[0], source_type=source_type)
        rest = ",\n".join(
            [sql_to_string(col, source_type=source_type) for col in columns[1:]]
        ).strip()
        alter_table_sql += f" ADD COLUMN {first}"
        if rest:
            alter_table_sql += f",\n{rest}"
    else:
        tuple_expr = expressions.Tuple(expressions=columns)
        alter_table_sql += " ADD COLUMNS " + sql_to_string(tuple_expr, source_type=source_type)

    return alter_table_sql


class FeatureTableCacheService:
    """
    Service for managing physical feature table cache as well as metadata storage.
    """

    def __init__(
        self,
        feature_table_cache_metadata_service: FeatureTableCacheMetadataService,
        namespace_handler: NamespaceHandler,
        session_manager_service: SessionManagerService,
        entity_validation_service: EntityValidationService,
        tile_cache_service: TileCacheService,
        feature_list_service: FeatureListService,
    ):
        self.feature_table_cache_metadata_service = feature_table_cache_metadata_service
        self.namespace_handler = namespace_handler
        self.session_manager_service = session_manager_service
        self.entity_validation_service = entity_validation_service
        self.tile_cache_service = tile_cache_service
        self.feature_list_service = feature_list_service

    async def get_non_cached_nodes(
        self,
        feature_table_cache_metadata: FeatureTableCacheMetadataModel,
        graph: QueryGraph,
        nodes: List[Node],
    ) -> List[Tuple[Node, CachedFeatureDefinition]]:
        """
        Given an observation table, graph and set of nodes
            - compute nodes definition hashes
            - lookup existing Feature Table Cache metadata
            - filter out nodes which are already added to the Feature Table Cache
            - return cached and non-cached nodes and their definition hashes.

        Parameters
        ----------
        feature_table_cache_metadata: FeatureTableCacheMetadataModel
            Feature table cache metadata
        graph: QueryGraph
            Graph definition
        nodes: List[Node]
            Input node names

        Returns
        -------
        List[Tuple[Node, CachedFeatureDefinition]]
            List of non cached nodes and respective newly-created cached feature definitions
        """
        node_names = [node.name for node in nodes]
        pruned_graph, node_name_map = graph.quick_prune(target_node_names=node_names)

        hashes = {}
        for node in nodes:
            (
                prepared_graph,
                prepared_node_name,
            ) = await self.namespace_handler.prepare_graph_to_store(
                graph=pruned_graph,
                node=pruned_graph.get_node_by_name(node_name_map[node.name]),
                sanitize_for_definition=True,
            )
            definition_hash_extractor = DefinitionHashExtractor(graph=prepared_graph)
            definition_hash = definition_hash_extractor.extract(
                prepared_graph.get_node_by_name(prepared_node_name)
            ).definition_hash
            hashes[definition_hash] = node

        cached_hashes = {
            feat.definition_hash: feat for feat in feature_table_cache_metadata.feature_definitions
        }

        # for non-cached features -extract definition hashes
        non_cached = [
            (
                node,
                CachedFeatureDefinition(definition_hash=definition_hash),
            )
            for definition_hash, node in hashes.items()
            if definition_hash not in cached_hashes
        ]

        return non_cached

    async def _populate_intermediate_table(
        self,
        feature_store: FeatureStoreModel,
        observation_table: ObservationTableModel,
        db_session: BaseSession,
        intermediate_table_name: str,
        graph: QueryGraph,
        nodes: List[Tuple[Node, CachedFeatureDefinition]],
        serving_names_mapping: Optional[Dict[str, str]] = None,
        is_feature_list_deployed: bool = False,
    ) -> None:
        request_column_names = {col.name for col in observation_table.columns_info}
        nodes_only = [node for node, _ in nodes]
        parent_serving_preparation = (
            await self.entity_validation_service.validate_entities_or_prepare_for_parent_serving(
                graph=graph,
                nodes=nodes_only,
                request_column_names=request_column_names,
                feature_store=feature_store,
                serving_names_mapping=serving_names_mapping,
            )
        )
        output_table_details = TableDetails(
            database_name=db_session.database_name,
            schema_name=db_session.schema_name,
            table_name=intermediate_table_name,
        )
        await get_historical_features(
            session=db_session,
            tile_cache_service=self.tile_cache_service,
            graph=graph,
            nodes=nodes_only,
            observation_set=observation_table,
            feature_store=feature_store,
            output_table_details=output_table_details,
            serving_names_mapping=serving_names_mapping,
            is_feature_list_deployed=is_feature_list_deployed,
            parent_serving_preparation=parent_serving_preparation,
        )

    async def _create_table(
        self,
        feature_store: FeatureStoreModel,
        observation_table: ObservationTableModel,
        db_session: BaseSession,
        final_table_name: str,
        graph: QueryGraph,
        nodes: List[Tuple[Node, CachedFeatureDefinition]],
        serving_names_mapping: Optional[Dict[str, str]] = None,
        is_feature_list_deployed: bool = False,
    ) -> None:
        intermediate_table_name = (
            f"__TEMP__{MaterializedTableNamePrefix.FEATURE_TABLE_CACHE}_{ObjectId()}"
        )
        try:
            await self._populate_intermediate_table(
                feature_store=feature_store,
                observation_table=observation_table,
                db_session=db_session,
                intermediate_table_name=intermediate_table_name,
                graph=graph,
                nodes=nodes,
                serving_names_mapping=serving_names_mapping,
                is_feature_list_deployed=is_feature_list_deployed,
            )

            feature_names = [
                expressions.alias_(
                    quoted_identifier(cast(str, graph.get_node_output_column_name(node.name))),
                    alias=quoted_identifier(cast(str, feature_definition.feature_name)),
                    quoted=True,
                )
                for node, feature_definition in nodes
            ]
            adapter = get_sql_adapter(db_session.source_type)
            query = sql_to_string(
                adapter.create_table_as(
                    table_details=TableDetails(
                        database_name=db_session.database_name,
                        schema_name=db_session.schema_name,
                        table_name=final_table_name,
                    ),
                    select_expr=(
                        expressions.select(quoted_identifier(InternalName.TABLE_ROW_INDEX))
                        .select(*feature_names)
                        .from_(quoted_identifier(intermediate_table_name))
                    ),
                ),
                source_type=db_session.source_type,
            )
            await db_session.execute_query(query)
        finally:
            await db_session.drop_table(
                database_name=db_session.database_name,
                schema_name=db_session.schema_name,
                table_name=intermediate_table_name,
                if_exists=True,
            )

    async def _update_table(
        self,
        feature_store: FeatureStoreModel,
        observation_table: ObservationTableModel,
        cache_metadata: FeatureTableCacheMetadataModel,
        db_session: BaseSession,
        graph: QueryGraph,
        non_cached_nodes: List[Tuple[Node, CachedFeatureDefinition]],
        serving_names_mapping: Optional[Dict[str, str]] = None,
        is_feature_list_deployed: bool = False,
    ) -> None:
        # create temporary table with features
        intermediate_table_name = (
            f"__TEMP__{MaterializedTableNamePrefix.FEATURE_TABLE_CACHE}_{ObjectId()}"
        )

        merge_target_table_alias = "feature_table_cache"
        merge_source_table_alias = "partial_features"

        try:
            await self._populate_intermediate_table(
                feature_store=feature_store,
                observation_table=observation_table,
                db_session=db_session,
                intermediate_table_name=intermediate_table_name,
                graph=graph,
                nodes=non_cached_nodes,
                serving_names_mapping=serving_names_mapping,
                is_feature_list_deployed=is_feature_list_deployed,
            )

            # alter cached tables adding columns for new features
            adapter = get_sql_adapter(db_session.source_type)
            table_exr = expressions.Table(
                this=quoted_identifier(cache_metadata.table_name),
                db=quoted_identifier(db_session.schema_name),
                catalog=quoted_identifier(db_session.database_name),
            )
            columns_expr = [
                expressions.ColumnDef(
                    this=quoted_identifier(cast(str, definition.feature_name)),
                    kind=adapter.get_physical_type_from_dtype(
                        extract_dtype_from_graph(graph, node)
                    ),
                )
                for node, definition in non_cached_nodes
            ]
            alter_table_sql = alter_table_add_columns_sql(
                table_exr, columns_expr, db_session.source_type
            )
            await db_session.execute_query(alter_table_sql)

            # merge temp table into cache table
            merge_conditions = [
                expressions.EQ(
                    this=get_qualified_column_identifier(
                        InternalName.TABLE_ROW_INDEX, merge_target_table_alias
                    ),
                    expression=get_qualified_column_identifier(
                        InternalName.TABLE_ROW_INDEX, merge_source_table_alias
                    ),
                )
            ]
            update_expr = expressions.Update(
                expressions=[
                    expressions.EQ(
                        this=get_qualified_column_identifier(
                            cast(str, definition.feature_name), merge_target_table_alias
                        ),
                        expression=get_qualified_column_identifier(
                            cast(str, graph.get_node_output_column_name(node.name)),
                            merge_source_table_alias,
                        ),
                    )
                    for node, definition in non_cached_nodes
                ]
            )
            merge_expr = expressions.Merge(
                this=expressions.Table(
                    this=quoted_identifier(cache_metadata.table_name),
                    db=quoted_identifier(db_session.schema_name),
                    catalog=quoted_identifier(db_session.database_name),
                    alias=expressions.TableAlias(this=merge_target_table_alias),
                ),
                using=expressions.Table(
                    this=quoted_identifier(intermediate_table_name),
                    db=quoted_identifier(db_session.schema_name),
                    catalog=quoted_identifier(db_session.database_name),
                    alias=expressions.TableAlias(this=merge_source_table_alias),
                ),
                on=expressions.and_(*merge_conditions),
                expressions=[
                    expressions.When(
                        this=expressions.Column(this=expressions.Identifier(this="MATCHED")),
                        then=update_expr,
                    ),
                ],
            )
            sql = sql_to_string(merge_expr, source_type=db_session.source_type)
            await db_session.execute_query(sql)
        finally:
            await db_session.drop_table(
                database_name=db_session.database_name,
                schema_name=db_session.schema_name,
                table_name=intermediate_table_name,
                if_exists=True,
            )

    async def create_or_update_feature_table_cache(
        self,
        feature_store: FeatureStoreModel,
        observation_table: ObservationTableModel,
        graph: QueryGraph,
        nodes: List[Node],
        feature_list_id: Optional[PydanticObjectId] = None,
        serving_names_mapping: Optional[Dict[str, str]] = None,
    ) -> None:
        """
        Create or update feature table cache

        Parameters
        ----------
        feature_store: FeatureStoreModel
            Feature Store object
        observation_table: ObservationTableModel
            Observation table object
        graph: QueryGraph
            Graph definition
        nodes: List[Node]
            Input node names
        feature_list_id: Optional[PydanticObjectId]
            Optional feature list id
        serving_names_mapping: Optional[Dict[str, str]]
            Optional serving names mapping if the observations set has different serving name columns
            than those defined in Entities
        """
        assert (
            observation_table.has_row_index
        ), "Observation Tables without row index are not supported"

        cache_metadata = (
            await self.feature_table_cache_metadata_service.get_or_create_feature_table_cache(
                observation_table_id=observation_table.id,
            )
        )
        feature_table_cache_exists = bool(cache_metadata.feature_definitions)

        non_cached_nodes = await self.get_non_cached_nodes(cache_metadata, graph, nodes)

        if non_cached_nodes:
            is_feature_list_deployed = False
            if feature_list_id:
                try:
                    feature_list = await self.feature_list_service.get_document(feature_list_id)
                    is_feature_list_deployed = feature_list.deployed
                except DocumentNotFoundError:
                    is_feature_list_deployed = False

            db_session = await self.session_manager_service.get_feature_store_session(
                feature_store=feature_store
            )

            if feature_table_cache_exists:
                # if feature table cache exists - update existing table with new features
                await self._update_table(
                    feature_store=feature_store,
                    observation_table=observation_table,
                    cache_metadata=cache_metadata,
                    db_session=db_session,
                    graph=graph,
                    non_cached_nodes=non_cached_nodes,
                    serving_names_mapping=serving_names_mapping,
                    is_feature_list_deployed=is_feature_list_deployed,
                )
            else:
                # if feature table doesn't exist yet - create from scratch
                await self._create_table(
                    feature_store=feature_store,
                    observation_table=observation_table,
                    db_session=db_session,
                    final_table_name=cache_metadata.table_name,
                    graph=graph,
                    nodes=non_cached_nodes,
                    serving_names_mapping=serving_names_mapping,
                    is_feature_list_deployed=is_feature_list_deployed,
                )

            await self.feature_table_cache_metadata_service.update_feature_table_cache(
                observation_table_id=observation_table.id,
                feature_definitions=[definition for _, definition in non_cached_nodes],
            )
