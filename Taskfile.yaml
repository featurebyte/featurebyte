version: '3'

env:
  FEAST_USAGE: False

includes:
  lint: Taskfile.lint.yaml

tasks:
  init:
    desc: "Initialize the project for development"
    preconditions:
      - sh: poetry --version
        msg: "Poetry is not installed. Please install it from https://python-poetry.org/docs/#installation"
      - sh: docker --version
        msg: "Docker is not installed. Please install it from https://docs.docker.com/get-docker/"
      - sh: yq --version
        msg: "yq is not installed. Please install it from https://github.com/mikefarah/yq"
      - sh: jq --version
        msg: "jq is not installed. Please install it from https://stedolan.github.io/jq/download/"
    deps:
      - task: install
    cmds:
      - poetry run pre-commit install

  install:
    desc: "Install the project dependencies"
    run: once
    sources:
      - pyproject.toml
      - poetry.lock
    deps:
      - task: build-jar
    cmds:
      - poetry install -n --sync --extras=server

  install-published-featurebyte:
    desc: "Install the published version of the featurebyte SDK"
    cmds:
      - poetry run pip uninstall --yes featurebyte
      - poetry run pip install featurebyte

  check-poetry:
    desc: "Check that pyproject.toml and poetry.lock are consistent"
    run: once
    sources:
      - pyproject.toml
      - poetry.lock
    cmds:
      - poetry check
      - poetry lock --check

  build-jar:
    desc: "Compile the hive jar files"
    run: once
    sources:
      - hive-udf/gradle/**/*
      - hive-udf/lib/src/**/*
      - hive-udf/lib/build.gradle
    generates:
      - featurebyte/sql/spark/*.jar
    cmds:
      - cd hive-udf && ./gradlew spotlessApply  # format-java
      - cd hive-udf && ./gradlew spotlessCheck  # lint-java
      - rm -f hive-udf/lib/build/libs/*.jar
      - cd hive-udf && ./gradlew shadowJar
      - cd hive-udf && ./gradlew test  # test-java
      - rm -f featurebyte/sql/spark/*.jar
      - cp hive-udf/lib/build/libs/*.jar featurebyte/sql/spark/

  test:
    desc: Runs full test-suite
    deps:
      - task: install
    cmds:
      - task: test-unit
      - task: test-integration
      - task: test-docs
      - task: generate-unit-test-fixtures

  test-unit:
    desc: Runs unit tests
    deps:
      - task: install
    sources:
      - poetry.lock
      - pyproject.toml
      - featurebyte/**/*
      - tests/**/*
    cmds:
      - task: test-setup
      - poetry run pytest --reruns=3 --timeout=240 --junitxml=pytest.xml.0 -n auto --cov=featurebyte tests/unit
      - task: test-teardown

  generate-unit-test-fixtures:
    desc: Generate unit test fixtures
    deps:
      - task: install
    sources:
      - poetry.lock
      - pyproject.toml
      - featurebyte/**/*
      - tests/unit/test_generate_payload_fixtures.py
    cmds:
      - poetry run pytest --timeout=240 --update-fixtures tests/unit/test_generate_payload_fixtures.py

  test-integration:
    desc: Runs integration tests
    deps:
      - task: install
    cmds:
      - task: test-integration-snowflake
      - task: test-integration-spark
      - task: test-integration-databricks
      - task: test-integration-databricks-unity
      - task: test-integration-bigquery

  test-integration-snowflake:
    desc: Runs integration tests against Snowflake
    deps:
      - task: install
    vars:
      # Default to 1 split 1 group
      PYTEST_GROUP:
        sh: printenv PYTEST_GROUP || echo "1"
      PYTEST_SPLITS:
        sh: printenv PYTEST_SPLITS || echo "1"
    env:
      MONGODB_URI: mongodb://localhost:37017/?replicaSet=rs0
      REDIS_URI: redis://localhost:36379
    cmds:
      - task: test-setup
      - poetry run pytest --timeout=360 --timeout-method=thread --junitxml=pytest.xml.1 --cov=featurebyte tests/integration --source-types none,snowflake --splits={{.PYTEST_SPLITS}} --group={{.PYTEST_GROUP}} --store-durations --clean-durations --ignore=tests/integration/worker/test_task_manager.py
      - task: test-teardown

  test-integration-task-manager:
    desc: Runs integration tests against Snowflake
    deps:
      - task: install
    vars:
      # Default to 1 split 1 group
      PYTEST_GROUP:
        sh: printenv PYTEST_GROUP || echo "1"
      PYTEST_SPLITS:
        sh: printenv PYTEST_SPLITS || echo "1"
    env:
      MONGODB_URI: mongodb://localhost:37017/?replicaSet=rs0
      REDIS_URI: redis://localhost:36379
    cmds:
      - task: test-setup
      - DISABLE_TASK_MANAGER_MOCK=1 poetry run pytest --timeout=360 --timeout-method=thread --junitxml=pytest.xml.1b --cov=featurebyte tests/integration/worker/test_task_manager.py --source-types none,snowflake --splits={{.PYTEST_SPLITS}} --group={{.PYTEST_GROUP}} --store-durations --clean-durations
      - task: test-teardown

  test-reset:
    cmds:
      - task: test-teardown
      - task: test-setup

  test-reset-java:
    cmds:
      - task: build-jar
      - task: test-reset

  test-integration-spark:
    desc: Runs integration tests against Spark
    vars:
      # Default to teardown
      TEARDOWN:
        sh: printenv TEARDOWN || echo "true"

      # Default to 1 split 1 group
      PYTEST_GROUP:
        sh: printenv PYTEST_GROUP || echo "1"
      PYTEST_SPLITS:
        sh: printenv PYTEST_SPLITS || echo "1"
    env:
      MONGODB_URI: mongodb://localhost:37017/?replicaSet=rs0
      REDIS_URI: redis://localhost:36379
    deps:
      - task: install
    cmds:
      - task: test-setup
      - poetry run pytest --timeout=900 --junitxml=pytest.xml.2 --cov=featurebyte tests/integration --source-types spark --splits={{.PYTEST_SPLITS}} --group={{.PYTEST_GROUP}} --store-durations --clean-durations
      - cmd: |
          if [ "{{.TEARDOWN}}" == "true" ]; then
              task test-teardown
          fi

  test-integration-databricks-unity:
    desc: Runs integration tests against Databricks
    deps:
      - task: install
    vars:
      # Default to 1 split 1 group
      PYTEST_GROUP:
        sh: printenv PYTEST_GROUP || echo "1"
      PYTEST_SPLITS:
        sh: printenv PYTEST_SPLITS || echo "1"
    env:
      MONGODB_URI: mongodb://localhost:37017/?replicaSet=rs0
      REDIS_URI: redis://localhost:36379
    cmds:
      - task: test-setup
      - poetry run pytest --timeout=1800 --junitxml=pytest.xml.4 --cov=featurebyte tests/integration --source-types databricks_unity --splits={{.PYTEST_SPLITS}} --group={{.PYTEST_GROUP}} --store-durations --clean-durations
      - task: test-teardown

  test-integration-bigquery:
    desc: Runs integration tests against BigQuery
    deps:
      - task: install
    vars:
      # Default to 1 split 1 group
      PYTEST_GROUP:
        sh: printenv PYTEST_GROUP || echo "1"
      PYTEST_SPLITS:
        sh: printenv PYTEST_SPLITS || echo "1"
    env:
      MONGODB_URI: mongodb://localhost:37017/?replicaSet=rs0
      REDIS_URI: redis://localhost:36379
    cmds:
      - task: test-setup
      - poetry run pytest --timeout=1800 --junitxml=pytest.xml.5 --cov=featurebyte tests/integration --source-types bigquery --splits={{.PYTEST_SPLITS}} --group={{.PYTEST_GROUP}} --store-durations --clean-durations
      - task: test-teardown

  test-docs:
    desc: Runs documentation tests
    sources:
      - featurebyte/**/*
    deps:
      - task: install
    cmds:
      - task: test-docs-setup
      - poetry run pytest --timeout=240 featurebyte
      - task: test-docs-teardown

  test-docs-setup:
    desc: "Setup the test environment for docs"
    deps:
      - task: docker-build
    cmds:
      - poetry run python scripts/test-docs-setup.py

  test-docs-teardown:
    desc: "Teardown the test environment for docs"
    cmds:
      - poetry run featurebyte stop

  test-merge:
    desc: Runs tests on merge
    cmds:
      - "echo 'coverage: platform' > pytest-coverage.txt"
      - poetry run coverage combine
      - poetry run coverage report >> pytest-coverage.txt
      - poetry run junitparser merge pytest.xml.* pytest.xml

  test-notebooks:
    desc: "Runs the notebook tests"
    cmds:
      - poetry run pytest --junitxml=pytest.xml tests/notebooks

  test-quick-start-notebooks:
    desc: "Runs the notebook tests"
    cmds:
      - poetry run pytest --junitxml=pytest-quick.xml -k quick tests/notebooks

  test-deep-dive-notebooks:
    desc: "Runs the notebook tests"
    cmds:
      - poetry run pytest --junitxml=pytest.xml -k deep tests/notebooks

  test-playground-notebooks:
    desc: "Runs the notebook tests"
    cmds:
      - poetry run pytest --junitxml=pytest.xml -k playground tests/notebooks

  test-setup:
    desc: "Setup the test environment"
    env:
      LOCAL_UID:
        sh: id -u
      LOCAL_GID:
        sh: id -g
      SPARK_VERSION: '{{default "3.5.0"}}'
    cmds:
      - echo "SPARK_VERSION=$SPARK_VERSION"
      - mkdir -p ~/.spark/data
      - docker compose -p featurebyte_docker -f docker/docker-compose.yml up -d
      - task: test-setup-status

  test-setup-status:
    desc: "Wait for the test environment to be ready"
    internal: true
    silent: true
    preconditions:
      - sh: docker compose ls | grep 'featurebyte_docker'
        msg: "Test environment is not running."
    cmds:
      -  |-
        for i in {1..30}; do
          [[ $(docker compose ls --format=json | jq '.[] | select(.Name == "featurebyte_docker") | .Status == "running(3)"') == "true" ]] && exit 0
          echo "Waiting for the test environment to be ready"
          sleep 2
        done

  test-teardown:
    desc: "Teardown the test environment"
    env:
      SPARK_VERSION: '{{default "3.5.0"}}'
    preconditions:
      - sh: docker compose ls | grep 'featurebyte_docker'
        msg: "Test environment is not running."
    cmds:
      - docker compose -p featurebyte_docker -f docker/docker-compose.yml down

  docs:
    desc: "Build the documentation and reload the browser"
    deps:
      - task: install
    env:
      PYTHONPATH:
        sh: echo "$(pwd)/docs/extensions"
      FB_DOCS_DEBUG_MODE: True
    cmds:
      - poetry run mkdocs serve --config-file mkdocs.yaml

  docs-persist-reference:
    desc: "Build the documentation and persist the docs locally."
    deps:
      - task: install
    env:
      PYTHONPATH:
        sh: echo "$(pwd)/docs/extensions"
    cmds:
      - poetry run mkdocs build --config-file mkdocs.yaml

  docs-dump-csv:
    desc: "Dump the documentation into a CSV file for easy browsing."
    deps:
      - task: install
    env:
      PYTHONPATH:
        sh: echo "$(pwd)/docs/extensions"

    cmds:
      - poetry run python featurebyte/common/documentation/extract_csv.py

  docker-build:
    desc: "Build the docker image"
    deps:
      - task: install
    cmds:
      - docker buildx build . -f docker/Dockerfile -t featurebyte-server:latest

  docker-dev:
    desc: "Starts featurebyte-server in development mode"
    deps:
      - task: install
    cmds:
      - task: docker-build
      - poetry run featurebyte start
      - poetry run featurebyte start spark

  docker-dev-stop:
    desc: "Stops featurebyte-server in development mode"
    cmds:
      - poetry run featurebyte stop

  changelog:
    desc: "Generate all changelog entries in .changelog"
    cmds:
      - cmd: pip show pyyaml 2>&1 >/dev/null || pip install pyyaml
        silent: true
      - python .changelog/changelog-gen.py

  changelog-pr:
    desc: "Generate changelog for PR"
    vars:
      clogs:
        sh: |
          git diff $(git merge-base origin/main HEAD): --no-renames --name-status -- .changelog/ | awk '{if ($1 != "D" && $2 ~ /.yaml$/ && $2 !~ /TEMPLATE[.]yaml$/) { print $2 }}' | sed 's-.changelog/--g' | xargs
    # Do not run if there are no changelog files
    status:
      - "[ '{{.clogs}}' == '' ]"
    cmds:
      - cmd: pip show pyyaml 2>&1 >/dev/null || pip install pyyaml
        silent: true
      - python .changelog/changelog-gen.py {{.clogs}}
