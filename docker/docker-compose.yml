services:
  mongodb:
    image: "mongo:6"
    ports:
      - 27017:27017  # Expose MongoDB port
    command:
      - /bin/bash
      - -c
      - |
        # Enable job control
        set -m

        # Start mongodb in the background
        mongod --dbpath /data/db --replSet rs0 --bind_ip_all &

        until mongosh --eval 'db.runCommand("ping").ok' --quiet; do
          echo "Waiting for mongodb to be listening"
          sleep 1
        done

        # Bootstrap if not already bootstrapped
        if ! mongosh --eval "rs.status()" 1>/dev/null 2>&1; then
          mongosh --eval 'rs.initiate({ "_id": "rs0", "version": 1, "members": [{ "_id": 1, "host": "127.0.0.1", "priority": 1}]})'
        fi

        # Attach back to session
        fg
    healthcheck:
      test: mongosh --eval 'db.runCommand("ping").ok' --quiet
      interval: 60s
      timeout: 10s
      start_period: 5s

  redis:
    image: "redis:6.2.5"
    ports:
      - 6379:6379

  spark-thrift:
    image: featurebyte/cluster-apache-spark:${SPARK_VERSION}
    ports:
      - "10009:10000"
      - "4049:4040"
    environment:
      - SPARK_MASTER=local
      - SPARK_LOCAL_IP=spark-thrift
      - "LOCAL_UID=${LOCAL_UID}"
      - "LOCAL_GID=${LOCAL_GID}"
    volumes:
      - ~/.spark/data/staging:/opt/spark/data/derby/staging
    command:
      - /bin/sh
      - -c
      - |
        /opt/spark/bin/spark-submit \
        --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 \
        --name Thrift JDBC/ODBC Server \
        --hiveconf hive.server2.thrift.port=10000 \
        --hiveconf hive.server2.thrift.bind.host=0.0.0.0 \
        --hiveconf derby.system.home=/opt/spark/data/derby \
        --master=local[2] \
        --conf spark.driver.memory=3G \
        --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
        --conf spark.sql.catalogImplementation=hive  \
        --conf spark.sql.warehouse.dir=file:///opt/spark/data/derby/warehouse/ \
        --conf spark.sql.hive.thriftServer.singleSession=false \
        --conf spark.hadoop.metastore.catalog.default=spark
    healthcheck:
      test: netstat -ltn | grep -c 10000
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
