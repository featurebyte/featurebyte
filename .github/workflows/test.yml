name: test
# SECRETS NAME                   | Description                     | PERMISSIONS
# ------------------------------ | ------------------------------- | ----------------------------------------------------
## SLACK_OAUTH                   | Slack App OAUTH Token           | [Slack                          ] write message
## SNOWFLAKE_PASSWORD            | Snowflake password              | [Snowflake                      ] password
## GIST_SECRET                   | Github Gist Token               | [Github | Gist                  ] Write
on:
  workflow_dispatch:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches:
      - main
permissions:
  contents: write
  repository-projects: read
  issues: write
  pull-requests: write
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref }}
  cancel-in-progress: true
env:
  # Snowflake
  SNOWFLAKE_USER: github
  SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
  SNOWFLAKE_ACCOUNT: "fm54506.us-central1.gcp"
  SNOWFLAKE_WAREHOUSE: COMPUTE_WH
  SNOWFLAKE_DATABASE: FEATUREBYTE_TESTING
  SNOWFLAKE_SCHEMA: PUBLIC
  SNOWFLAKE_SCHEMA_FEATUREBYTE: FEATUREBYTE
  # Databricks
  DATABRICKS_ACCESS_TOKEN: ${{ secrets.DATABRICKS_ACCESS_TOKEN }}
  DATABRICKS_SERVER_HOSTNAME: ${{ secrets.DATABRICKS_SERVER_HOSTNAME }}
  DATABRICKS_HTTP_PATH: "sql/protocolv1/o/2085793316075774/0319-021506-hzmupduq"
  DATABRICKS_CATALOG: "hive_metastore"
  DATABRICKS_SCHEMA_FEATUREBYTE: "FEATUREBYTE_GITHUB"
  DATABRICKS_STORAGE_URL: ${{ secrets.DATABRICKS_STORAGE_URL }}
  DATABRICKS_STORAGE_ACCESS_KEY_ID: ${{ secrets.DATABRICKS_S3_ACCESS_KEY }}
  DATABRICKS_STORAGE_ACCESS_KEY_SECRET: ${{ secrets.DATABRICKS_S3_SECRET_KEY }}

jobs:
  build-jar:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v3
      - uses: arduino/setup-task@v1
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Setup gradle cache
        uses: actions/cache@v3
        with:
          path: hive-udf/.gradle
          key: gradle-cache-${{ runner.os }}
      - name: Setup gradle build cache
        uses: actions/cache@v3
        with:
          path: hive-udf/lib/build
          key: gradle-build-cache-${{ runner.os }}
      - name: Building hive-udf
        run: |
          task build-jar
      - name: Upload hive-udf
        uses: actions/upload-artifact@v3
        with:
          name: hive-udf
          path: hive-udf/lib/build/libs/*
  test-unit:
    needs: ["build-jar"]
    runs-on: ubuntu-20.04
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: ["3.8.0"]
    steps:
      - uses: actions/checkout@v3
      - uses: arduino/setup-task@v1
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Setup cache
        uses: actions/cache@v3
        with:
          path: .venv
          key: poetry-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ github.job }}
          restore-keys: poetry-cache-${{ runner.os }}-${{ matrix.python-version }} # Restore from `cache` job
      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 1.3.1
      - name: Configure Poetry
        run: |
          poetry config virtualenvs.in-project true
      - name: Adding C headers
        run: |
          sudo apt-get install python-dev libsasl2-dev gcc
          sudo apt-get install wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v3
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        run: |
          task test-unit
          mv .coverage .coverage.0
      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.0
          path: pytest.xml.0
      - name: Upload unit coverage results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.0
          path: .coverage.0
  test-integration-snowflake:
    needs: ["build-jar"]
    runs-on: ubuntu-20.04
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: ["3.8.0"]
    steps:
      - uses: actions/checkout@v3
      - uses: arduino/setup-task@v1
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Setup cache
        uses: actions/cache@v3
        with:
          path: .venv
          key: poetry-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ github.job }}
          restore-keys: poetry-cache-${{ runner.os }}-${{ matrix.python-version }} # Restore from `cache` job
      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 1.3.1
      - name: Configure Poetry
        run: |
          poetry config virtualenvs.in-project true
      - name: Adding C headers + Install dependencies
        run: |
          sudo apt-get install python-dev libsasl2-dev gcc
          sudo apt-get install wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v3
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        run: |
          task test-integration-snowflake
          mv .coverage .coverage.1
      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.1
          path: pytest.xml.1
      - name: Upload integration coverage results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.1
          path: .coverage.1
  test-integration-spark:
    needs: ["build-jar"]
    runs-on: ubuntu-20.04
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: ["3.8.0"]
    steps:
      - uses: actions/checkout@v3
      - uses: arduino/setup-task@v1
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Setup cache
        uses: actions/cache@v3
        with:
          path: .venv
          key: poetry-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ github.job }}
          restore-keys: poetry-cache-${{ runner.os }}-${{ matrix.python-version }} # Restore from `cache` job
      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 1.3.1
      - name: Configure Poetry
        run: |
          poetry config virtualenvs.in-project true
      - name: Adding C headers
        run: |
          sudo apt-get install python-dev libsasl2-dev gcc
          sudo apt-get install wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v3
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        continue-on-error: true
        run: |
          set +e
          task test-integration-spark
          # capture the exit code from the test run before stashing logs
          EXIT_CODE=$?
          echo $EXIT_CODE > status.txt
          docker logs spark-thrift 2>&1 > spark-thrift.log
          # bail out now if the test run was a failure
          if [ $EXIT_CODE -ne 0 ]; then
            exit $EXIT_CODE
          fi
          task test-teardown
          mv .coverage .coverage.2
      - name: Upload spark-thrift log file
        uses: actions/upload-artifact@v3
        with:
          name: spark-thrift-logs
          path: spark-thrift.log
          retention-days: 5
      - name: Check test status
        run: |
          EXIT_CODE=$(cat status.txt)
          if [ $EXIT_CODE -ne 0 ]; then
            exit $EXIT_CODE
          fi
      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.2
          path: pytest.xml.2
      - name: Upload integration coverage results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.2
          path: .coverage.2
  test-integration-databricks:
    needs: ["build-jar"]
    runs-on: ubuntu-20.04
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: ["3.8.0"]
    steps:
      - uses: actions/checkout@v3
      - uses: arduino/setup-task@v1
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Setup cache
        uses: actions/cache@v3
        with:
          path: .venv
          key: poetry-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ github.job }}
          restore-keys: poetry-cache-${{ runner.os }}-${{ matrix.python-version }} # Restore from `cache` job
      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 1.3.1
      - name: Configure Poetry
        run: |
          poetry config virtualenvs.in-project true
      - name: Adding C headers
        run: |
          sudo apt-get install python-dev libsasl2-dev gcc
          sudo apt-get install wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v3
        with:
          name: hive-udf
          path: featurebyte/sql/databricks/
      - name: Run tests
        run: |
          task test-integration-databricks
          mv .coverage .coverage.3
      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.3
          path: pytest.xml.3
      - name: Upload integration coverage results
        uses: actions/upload-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.3
          path: .coverage.3
  test-docs:
    needs: ["build-jar"]
    runs-on: ubuntu-20.04
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: [ "3.8.0" ]
    steps:
      - uses: actions/checkout@v3
      - uses: arduino/setup-task@v1
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Setup cache
        uses: actions/cache@v3
        with:
          path: .venv
          key: poetry-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ github.job }}
          restore-keys: poetry-cache-${{ runner.os }}-${{ matrix.python-version }} # Restore from `cache` job
      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 1.3.1
      - name: Configure Poetry
        run: |
          poetry config virtualenvs.in-project true
      - name: Adding C headers and wkhtmltopdf
        run: |
          sudo apt-get install python-dev libsasl2-dev gcc
          sudo apt-get install wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v3
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Test documentation
        run: |
          task test-docs
  validate:
    needs: ["test-unit", "test-integration-snowflake", "test-docs", "test-integration-spark", "test-integration-databricks"]
    runs-on: ubuntu-20.04
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: ["3.8.0"]
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: '0' # Used for checking diff
      - uses: arduino/setup-task@v1
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Setup cache
        uses: actions/cache@v3
        with:
          path: .venv
          key: poetry-cache-${{ runner.os }}-${{ matrix.python-version }}-${{ github.job }}
          restore-keys: poetry-cache-${{ runner.os }}-${{ matrix.python-version }} # Restore from `cache` job
      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 1.3.1
      - name: Configure Poetry
        run: |
          poetry config virtualenvs.in-project true
      - name: Install dependencies
        run: |
          poetry install -n --sync
      - name: Download unit test results
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.0
      - name: Download integration test results (snowflake)
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.1
      - name: Download integration test results (spark)
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.2
      - name: Download integration test results (databricks)
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.3
      - name: Download unit coverage results
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.0
      - name: Download integration coverage results (snowflake)
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.1
      - name: Download integration coverage results (spark)
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.2
      - name: Download integration coverage results (databricks)
        uses: actions/download-artifact@v3
        with:
          name: python-${{ matrix.python-version }}-.coverage.3
      - name: Merge test results
        run: |
          task test-merge
      - name: Test Coverage Report
        id: coverageComment
        uses: MishaKav/pytest-coverage-comment@main
        with:
          pytest-coverage-path: pytest-coverage.txt
          junitxml-path: pytest.xml
      - name: Check test output
        run: |
          if [ "${{ steps.coverageComment.outputs.errors }}" -gt '0' ] || [ "${{ steps.coverageComment.outputs.failures }}" -gt '0' ]; then
            exit 1
          fi
      - name: Update Coverage Badge
        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
        uses: schneegans/dynamic-badges-action@v1.6.0
        with:
          auth: ${{ secrets.GIST_SECRET }}
          gistID: 773e2960183c0a6fe24c644d95d71fdb
          filename: coverage.json
          label: coverage
          message: ${{ steps.coverageComment.outputs.coverage }}
          color: ${{ steps.coverageComment.outputs.color }}
  slack:
    runs-on: ubuntu-latest
    if: ${{ always() }}
    needs: ["build-jar", "test-unit", "test-integration-snowflake", "test-integration-spark", "test-integration-databricks", "test-docs", "validate"]
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: Get Author Email (PR)
        if: ${{ github.event_name == 'pull_request' }}
        run: echo "AUTHOR_EMAIL=$(git show -s --format='%ae' "origin/${GITHUB_HEAD_REF}")" >> $GITHUB_ENV
      - name: Get Author Email (PUSH)
        if: ${{ github.event_name != 'pull_request' }}
        run: echo "AUTHOR_EMAIL=$(git show -s --format='%ae' "origin/${GITHUB_REF_NAME}")" >> $GITHUB_ENV
      - name: Get Channel ID
        run: |
          export AUTHOR_ID=`curl -X POST -H "Authorization: Bearer ${{ secrets.SLACK_OAUTH }}" -H 'Content-type: application/json; charset=utf-8' https://slack.com/api/users.list | grep -oP $(echo '"id":"\K[^"]+?"(?:(?!"id").)*${AUTHOR_EMAIL}' | envsubst) | grep -oP '^[^"]+'`
          echo "CHANNEL_ID=${AUTHOR_ID}" >> $GITHUB_ENV
      - name: Get Results
        run: |
          echo "BUILD_JAR_RESULT=${{ needs.build-jar.result }}" >> $GITHUB_ENV
          echo "TEST_UNIT_RESULT=${{ needs.test-unit.result }}" >> $GITHUB_ENV
          echo "TEST_INTEG_RESULT=${{ needs.test-integration-snowflake.result }},${{ needs.test-integration-spark.result }},${{ needs.test-integration-databricks.result }}" >> $GITHUB_ENV
          echo "TEST_DOCS_RESULT=${{ needs.test-docs.result }}" >> $GITHUB_ENV
          echo "VALIDATE_RESULT=${{ needs.validate.result }}" >> $GITHUB_ENV
          echo "REPOSITORY=$(echo '${{ github.repository }}' | cut -d / -f2)" >> $GITHUB_ENV
      - name: Send Slack notification with workflow result.
        if: ${{ env.CHANNEL_ID != '' }}
        uses: slackapi/slack-github-action@v1.23.0
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_OAUTH }}
        with:
          channel-id: ${{ env.CHANNEL_ID }}
          payload: |
            {
              "text": "${{ env.REPOSITORY }}[${{ github.workflow }}] [${{ env.BUILD_JAR_RESULT }}, ${{ env.TEST_UNIT_RESULT }}, ${{ env.TEST_INTEG_RESULT }}, ${{ env.TEST_DOCS_RESULT}}, ${{ env.VALIDATE_RESULT }}]",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "`${{ env.REPOSITORY }} [${{ github.workflow }}]`: ${{ github.event.pull_request.html_url || github.event.head_commit.url }}"
                  }
                }, {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "```build_jar: [${{ env.BUILD_JAR_RESULT }}]\n--> test_unit [${{ env.TEST_UNIT_RESULT }}]\n--> test_integration [${{ env.TEST_INTEG_RESULT }}]\n--> test_docs [${{ env.TEST_DOCS_RESULT }}]\n----> validate [${{ env.VALIDATE_RESULT }}]```"
                  }
                }
              ]
            }
      - name: Send Slack notification to Developer channel if Pushed to main and failed
        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' && contains(needs.*.result, 'failure') }}
        uses: slackapi/slack-github-action@v1.23.0
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_OAUTH }}
        with:
          channel-id: C03D81HC6Q5
          payload: |
            {
              "text": "${{ env.REPOSITORY }}[${{ github.workflow }}] [${{ env.BUILD_JAR_RESULT }}, ${{ env.TEST_UNIT_RESULT }}, ${{ env.TEST_INTEG_RESULT }}, ${{ env.TEST_DOCS_RESULT}}, ${{ env.VALIDATE_RESULT }}]",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "`${{ env.REPOSITORY }} [${{ github.workflow }}]`: ${{ github.event.pull_request.html_url || github.event.head_commit.url }}"
                  }
                }, {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "```build_jar: [${{ env.BUILD_JAR_RESULT }}]\n--> test_unit [${{ env.TEST_UNIT_RESULT }}]\n--> test_integration [${{ env.TEST_INTEG_RESULT }}]\n--> test_docs [${{ env.TEST_DOCS_RESULT }}]\n----> validate [${{ env.VALIDATE_RESULT }}]```"
                  }
                }
              ]
            }
