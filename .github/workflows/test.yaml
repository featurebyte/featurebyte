name: test
'on':
  workflow_dispatch: { }
  pull_request:
    types:
      - opened
      - synchronize
      - reopened
    branches:
      - main
      - stable/*
      - release/*
  push:
    branches:
      - main
permissions:
  contents: write
  pull-requests: write
  repository-projects: read
  issues: write
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref }}
  cancel-in-progress: true
env:
  SNOWFLAKE_USER: github
  SNOWFLAKE_PRIVATE_KEY: ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
  SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
  SNOWFLAKE_ACCOUNT: fm54506.us-central1.gcp
  SNOWFLAKE_WAREHOUSE: COMPUTE_WH
  SNOWFLAKE_DATABASE: FEATUREBYTE_TESTING
  SNOWFLAKE_SCHEMA: PUBLIC
  SNOWFLAKE_SCHEMA_FEATUREBYTE: FEATUREBYTE
  DATABRICKS_ACCESS_TOKEN: ${{ secrets.DATABRICKS_ACCESS_TOKEN }}
  DATABRICKS_CLIENT_ID: ${{ secrets.DATABRICKS_CLIENT_ID }}
  DATABRICKS_CLIENT_SECRET: ${{ secrets.DATABRICKS_CLIENT_SECRET }}
  DATABRICKS_SERVER_HOSTNAME: ${{ secrets.DATABRICKS_SERVER_HOSTNAME }}
  DATABRICKS_HTTP_PATH: ${{ secrets.DATABRICKS_HTTP_PATH }}
  DATABRICKS_CATALOG: hive_metastore
  DATABRICKS_UNITY_HTTP_PATH: ${{ secrets.DATABRICKS_UNITY_HTTP_PATH }}
  DATABRICKS_UNITY_CATALOG: ${{ secrets.DATABRICKS_UNITY_CATALOG }}
  DATABRICKS_SCHEMA_FEATUREBYTE: FEATUREBYTE_GITHUB
  DATABRICKS_STORAGE_URL: ${{ secrets.DATABRICKS_STORAGE_URL }}
  S3_ACCESS_KEY: ${{ secrets.S3_ACCESS_KEY }}
  S3_SECRET_KEY: ${{ secrets.S3_SECRET_KEY }}
  GCS_CLOUD_STORAGE_RW_TEST: ${{ secrets.GCS_CLOUD_STORAGE_RW_TEST }}
  AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
  AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
  BIGQUERY_PROJECT: ${{ secrets.BIGQUERY_PROJECT }}
  BIGQUERY_SERVICE_ACCOUNT_INFO: ${{ secrets.BIGQUERY_SERVICE_ACCOUNT_INFO }}
jobs:
  build-jar:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Setup gradle cache
        uses: actions/cache@v4
        with:
          path: hive-udf/.gradle
          key: gradle-cache-${{ runner.os }}
      - name: Setup gradle build cache
        uses: actions/cache@v4
        with:
          path: hive-udf/lib/build
          key: gradle-build-cache-${{ runner.os }}
      - name: Building hive-udf
        run: task build-jar
      - name: Upload hive-udf
        uses: actions/upload-artifact@v4.5.0
        with:
          name: hive-udf
          path: hive-udf/lib/build/libs/*
  test-unit:
    needs:
      - build-jar
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        python-version:
          - 3.10.14
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          sudo apt-get update
          sudo apt-get install libkrb5-dev libsasl2-dev libpython3-dev g++ gcc wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v4
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        run: |-
          task test-unit
          task generate-unit-test-fixtures
      - name: Renaming test assets
        run: mv .coverage .coverage.0
      - name: Upload test results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.0
          path: pytest.xml.0
      - name: Upload coverage results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.coverage.0
          path: .coverage.0
          include-hidden-files: true
  test-integration-snowflake:
    needs:
      - build-jar
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        python-version:
          - 3.10.14
        pytest-group:
          - 1
          - 2
    env:
      PYTEST_SPLITS: 2  # This needs to be EQ the count of strategy.matrix.pytest-group
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          sudo apt-get update
          sudo apt-get install libkrb5-dev libsasl2-dev libpython3-dev g++ gcc wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v4
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        env:
          PYTEST_GROUP: ${{ matrix.pytest-group }}
        run: task test-integration-snowflake
      - name: Renaming test assets
        run: |
          mv .coverage .coverage.1.${{ matrix.pytest-group }}
          mv pytest.xml.1 pytest.xml.1.${{ matrix.pytest-group }}
          mv .test_durations .test_durations.1.${{ matrix.pytest-group }}
      - name: Upload test results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.1.${{ matrix.pytest-group }}
          path: pytest.xml.1.${{ matrix.pytest-group }}
          overwrite: true
      - name: Upload coverage results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.coverage.1.${{ matrix.pytest-group }}
          path: .coverage.1.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true
      - name: Upload test durations
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.test_durations.1.${{ matrix.pytest-group }}
          path: .test_durations.1.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true
  test-integration-task-manager:
    needs:
      - build-jar
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        python-version:
          - 3.10.14
        pytest-group:
          - 1
    env:
      PYTEST_SPLITS: 1  # This needs to be EQ the count of strategy.matrix.pytest-group
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          sudo apt-get update
          sudo apt-get install libkrb5-dev libsasl2-dev libpython3-dev g++ gcc
      - name: Install wkhtmltopdf
        run: |-
          sudo apt-get update
          sudo apt-get install wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v4
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        env:
          PYTEST_GROUP: ${{ matrix.pytest-group }}
        run: task test-integration-task-manager
      - name: Renaming test assets
        run: |
          mv .coverage .coverage.1b.${{ matrix.pytest-group }}
          mv pytest.xml.1b pytest.xml.1b.${{ matrix.pytest-group }}
          mv .test_durations .test_durations.1b.${{ matrix.pytest-group }}
      - name: Upload test results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.1b.${{ matrix.pytest-group }}
          path: pytest.xml.1b.${{ matrix.pytest-group }}
          overwrite: true
      - name: Upload coverage results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.coverage.1b.${{ matrix.pytest-group }}
          path: .coverage.1b.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true
      - name: Upload test durations
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.test_durations.1b.${{ matrix.pytest-group }}
          path: .test_durations.1b.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true
  test-integration-spark:
    needs:
      - build-jar
    runs-on:
      group: Public Runners
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        python-version:
          - 3.10.14
        spark-version:
          - 3.5.0
        pytest-group:
          - 1
          - 2
          - 3
    env:
      PYTEST_SPLITS: 3  # This needs to be EQ the count of strategy.matrix.pytest-group
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          sudo apt-get update
          sudo apt-get install libkrb5-dev libsasl2-dev libpython3-dev g++ gcc wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v4
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        continue-on-error: true
        env:
          PYTEST_GROUP: ${{ matrix.pytest-group }}
        run: |-
          set +e
          export TEARDOWN=false  # Override teardown behavior
          export SPARK_VERSION=${{ matrix.spark-version }}
          task test-integration-spark

          # capture the exit code from the test run before stashing logs
          echo "SPARK_INTEGRATION_TEST_STATUS_CODE=$?" >> $GITHUB_ENV

          # Capturing spark logs
          docker logs spark-thrift 2>&1 > spark-thrift-${{ matrix.python-version }}-${{ matrix.spark-version }}-${{ matrix.pytest-group }}.log

          # Teardown
          task test-teardown
      - name: Renaming test assets
        run: |
          mv .coverage .coverage.2.${{ matrix.pytest-group }}
          mv pytest.xml.2 pytest.xml.2.${{ matrix.pytest-group }}
          mv .test_durations .test_durations.2.${{ matrix.pytest-group }}
      - name: Upload spark-thrift log file
        uses: actions/upload-artifact@v4.5.0
        with:
          name: spark-thrift-logs-${{ matrix.python-version }}-${{ matrix.spark-version }}-${{ matrix.pytest-group }}
          path: spark-thrift-${{ matrix.python-version }}-${{ matrix.spark-version }}-${{ matrix.pytest-group }}.log
          retention-days: 5

      # NOTE: Only holding 1 test result to be used in pytest report
      - name: Upload test results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.2.${{ matrix.pytest-group }}
          path: pytest.xml.2.${{ matrix.pytest-group }}
          overwrite: true
      - name: Upload coverage results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.coverage.2.${{ matrix.pytest-group }}
          path: .coverage.2.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true
      - name: Upload test durations
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.test_durations.2.${{ matrix.pytest-group }}
          path: .test_durations.2.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true

      # Fail the job if the test run failed
      - name: Check test status
        run: |
          exit ${{ env.SPARK_INTEGRATION_TEST_STATUS_CODE }}

  test-integration-databricks-unity:
    needs:
      - build-jar
    runs-on: ubuntu-latest
    timeout-minutes: 135
    strategy:
      fail-fast: false
      matrix:
        python-version:
          - 3.10.14
        pytest-group:
          - 1
          - 2
          - 3
          - 4
          - 5
    env:
      PYTEST_SPLITS: 5  # This needs to be EQ the count of strategy.matrix.pytest-group
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          sudo apt-get update
          sudo apt-get install libkrb5-dev libsasl2-dev libpython3-dev g++ gcc wkhtmltopdf
      - name: Run tests
        env:
          PYTEST_GROUP: ${{ matrix.pytest-group }}
        run: task test-integration-databricks-unity
      - name: Renaming test assets
        run: |
          mv .coverage .coverage.4.${{ matrix.pytest-group }}
          mv pytest.xml.4 pytest.xml.4.${{ matrix.pytest-group }}
          mv .test_durations .test_durations.4.${{ matrix.pytest-group }}
      - name: Upload test results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.4.${{ matrix.pytest-group }}
          path: pytest.xml.4.${{ matrix.pytest-group }}
      - name: Upload coverage results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.coverage.4.${{ matrix.pytest-group }}
          path: .coverage.4.${{ matrix.pytest-group }}
          include-hidden-files: true
      - name: Upload test durations
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.test_durations.4.${{ matrix.pytest-group }}
          path: .test_durations.4.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true

  test-integration-bigquery:
    needs:
      - build-jar
    runs-on: ubuntu-latest
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        python-version:
          - 3.10.14
        pytest-group:
          - 1
          - 2
          - 3
    env:
      PYTEST_SPLITS: 3  # This needs to be EQ the count of strategy.matrix.pytest-group
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          sudo apt-get update
          sudo apt-get install libkrb5-dev libsasl2-dev libpython3-dev g++ gcc wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v4
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Run tests
        env:
          PYTEST_GROUP: ${{ matrix.pytest-group }}
        run: task test-integration-bigquery
      - name: Renaming test assets
        run: |
          mv .coverage .coverage.5.${{ matrix.pytest-group }}
          mv pytest.xml.5 pytest.xml.5.${{ matrix.pytest-group }}
          mv .test_durations .test_durations.5.${{ matrix.pytest-group }}
      - name: Upload test results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-pytest.xml.5.${{ matrix.pytest-group }}
          path: pytest.xml.5.${{ matrix.pytest-group }}
          overwrite: true
      - name: Upload coverage results
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.coverage.5.${{ matrix.pytest-group }}
          path: .coverage.5.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true
      - name: Upload test durations
        uses: actions/upload-artifact@v4.5.0
        with:
          name: python-${{ matrix.python-version }}-.test_durations.5.${{ matrix.pytest-group }}
          path: .test_durations.5.${{ matrix.pytest-group }}
          overwrite: true
          include-hidden-files: true

  test-docs:
    needs:
      - build-jar
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        python-version:
          - 3.10.14
    steps:
      - uses: actions/checkout@v6
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          sudo apt-get update
          sudo apt-get install libkrb5-dev libsasl2-dev libpython3-dev g++ gcc wkhtmltopdf
      - name: Download hive-udf
        uses: actions/download-artifact@v4
        with:
          name: hive-udf
          path: featurebyte/sql/spark/
      - name: Test documentation
        run: |
          task test-docs-setup
          uv run pytest --timeout=240 featurebyte
  validate:
    needs:
      - test-unit
      - test-integration-spark
      - test-integration-databricks-unity
      - test-integration-snowflake
      - test-integration-bigquery
      - test-docs
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        python-version:
          - 3.10.14
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Install uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.python-version }}
          enable-cache: true
      - name: Install packages needed for build
        run: |-
          task install
      - name: Download integration test results (databricks-unity)
        uses: actions/download-artifact@v4
        with:
          pattern: python-${{ matrix.python-version }}-pytest.xml.*
          merge-multiple: true
      - name: Download integration coverage results
        uses: actions/download-artifact@v4
        with:
          pattern: python-${{ matrix.python-version }}-.coverage.*
          merge-multiple: true
      - name: Merge test results
        run: task test-merge
      - name: Upload Coverage Report
        uses: actions/upload-artifact@v4.5.0
        with:
          name: pytest-coverage.txt
          path: pytest-coverage.txt
      - name: Test Coverage Report
        id: coverageComment
        uses: MishaKav/pytest-coverage-comment@main
        with:
          pytest-coverage-path: pytest-coverage.txt
          junitxml-path: pytest.xml
          hide-report: true
      - name: Update Coverage Badge
        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
        uses: schneegans/dynamic-badges-action@v1.7.0
        with:
          auth: ${{ secrets.GIST_SECRET }}
          gistID: 773e2960183c0a6fe24c644d95d71fdb
          filename: coverage.json
          label: coverage
          message: ${{ steps.coverageComment.outputs.coverage }}
          color: ${{ steps.coverageComment.outputs.color }}
  combine-and-upload-test-durations:
    needs:
      - test-unit
      - test-integration-spark
      - test-integration-databricks-unity
      - test-integration-snowflake
      - test-integration-bigquery
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        python-version:
          - 3.10.14
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
      - uses: arduino/setup-task@v2
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 3.x
      - name: Download test durations
        uses: actions/download-artifact@v4
        with:
          pattern: python-${{ matrix.python-version }}-.test_durations.*
          merge-multiple: true
      - name: Merge test durations
        run: cat .test_durations.* | jq --slurp --sort-keys add > .test_durations
      - name: Upload combined test durations
        uses: actions/upload-artifact@v4.5.0
        with:
          name: .test_durations
          path: .test_durations
          overwrite: true
          include-hidden-files: true
  slack:
    needs:
      - test-unit
      - test-integration-snowflake
      - test-integration-spark
      - test-integration-databricks-unity
      - test-integration-bigquery
      - test-docs
      - validate
    if: ${{ always() }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
      - if: ${{ github.event_name == 'pull_request' }}
        name: Get Author Email (PR)
        run: echo "AUTHOR_EMAIL=$(git show -s --format='%ae' "origin/${{ github.head_ref }}")" >> $GITHUB_ENV
      - if: ${{ github.event_name != 'pull_request' }}
        name: Get Author Email (PUSH)
        run: echo "AUTHOR_EMAIL=$(git show -s --format='%ae' "origin/${{ github.ref_name }}")" >> $GITHUB_ENV
      - name: Get Channel ID
        run: |-
          echo "${{ secrets.SLACK_USERS }}" > users.csv
          export AUTHOR_ID="$(grep -oP "${{ env.AUTHOR_EMAIL }},\s*\K[^,]*" users.csv)"
          if [ "${AUTHOR_ID}" = "" ]; then
            echo "No Slack user found for ${AUTHOR_EMAIL}"
            export AUTHOR_ID="C03D81HC6Q5"  # Developer Channel
            exit 1
          fi
          echo "CHANNEL_ID=${AUTHOR_ID}" >> $GITHUB_ENV
      - name: Set Environment
        run: |
          echo "REPOSITORY=$(echo '${{ github.repository }}' | cut -d / -f2)" >> $GITHUB_ENV
      - name: Send Slack notification with workflow result.
        uses: slackapi/slack-github-action@v2.1.0
        with:
          method: chat.postMessage
          token: ${{ secrets.SLACK_OAUTH }}
          payload: |
            channel: ${{ env.CHANNEL_ID }}
            text: "`${{ env.REPOSITORY }} [${{ github.workflow }}]`: ${{ github.event.pull_request.html_url || github.event.head_commit.url }}"
            blocks:
              - type: section
                text:
                  type: mrkdwn
                  text: "`${{ env.REPOSITORY }} [${{ github.workflow }}]`: ${{ github.event.pull_request.html_url || github.event.head_commit.url }}"
              - type: section
                text:
                  type: mrkdwn
                  text: |-
                    ```
                    test_unit: [${{ needs.test-unit.result }}]
                    test_docs: [${{ needs.test-docs.result }}]
                    test_integration:
                      snowflake:        [${{ needs.test-integration-snowflake.result }}]
                      spark:            [${{ needs.test-integration-spark.result }}]
                      databricks-unity: [${{ needs.test-integration-databricks-unity.result }}]
                      bigquery:         [${{ needs.test-integration-bigquery.result }}]

                    validate: [${{ needs.validate.result }}]
                    ```
